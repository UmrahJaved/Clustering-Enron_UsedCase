{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling Q&D pilot\n",
    "https://medium.datadriveninvestor.com/nlp-with-lda-analyzing-topics-in-the-enron-email-dataset-20326b7ae36f\n",
    "https://github.com/shoreason/enron-topic-modeling/blob/master/enron_lda.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small set of emails (10 000)\n",
    "emails_read_all = pd.read_csv('/home/becode2/enron_DF/Clustering-Enron_UsedCase/emails.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron = emails_read_all.head(1000)#enron = pd.read_csv('./csv/emails_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_email(msg: email.message.Message) -> str:\n",
    "    \"\"\"To get the content from email objects\n",
    "    :param msg: the email object\n",
    "    :return: the content of the email\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_email_addresses(line: str) -> frozenset:\n",
    "    \"\"\"\n",
    "    To separate multiple email addresses\n",
    "    :param line: the line of the csv file\n",
    "    :return: a set of email addresses\n",
    "    \"\"\"\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enron.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/becode2/.local/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/tmp/ipykernel_169795/3640715177.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  enron[key] = [doc[key] for doc in messages]\n",
      "/tmp/ipykernel_169795/3640715177.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  enron['content'] = list(map(get_text_from_email, messages))\n",
      "/tmp/ipykernel_169795/3640715177.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  enron['From'] = enron['From'].map(split_email_addresses)\n",
      "/tmp/ipykernel_169795/3640715177.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  enron['To'] = enron['To'].map(split_email_addresses)\n",
      "/tmp/ipykernel_169795/3640715177.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  enron['user'] = enron['file'].map(lambda x:x.split('/')[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, enron['message']))\n",
    "enron.drop('message', axis=1, inplace=True)\n",
    "\n",
    "# Get fields from parsed email objects\n",
    "keys = messages[0].keys()\n",
    "for key in keys:\n",
    "    enron[key] = [doc[key] for doc in messages]\n",
    "\n",
    "# Parse content from emails\n",
    "enron['content'] = list(map(get_text_from_email, messages))\n",
    "\n",
    "# Split multiple email addresses\n",
    "enron['From'] = enron['From'].map(split_email_addresses)\n",
    "enron['To'] = enron['To'].map(split_email_addresses)\n",
    "\n",
    "# Extract the root of 'file' as 'user'\n",
    "enron['user'] = enron['file'].map(lambda x:x.split('/')[0])\n",
    "del messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['file', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Mime-Version',\n",
      "       'Content-Type', 'Content-Transfer-Encoding', 'X-From', 'X-To', 'X-cc',\n",
      "       'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName', 'content', 'user'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/becode2/.local/lib/python3.8/site-packages/pandas/core/frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "enron.shape #10 000 rows, 18 columns\n",
    "print(enron.columns)\n",
    "enron.rename(columns={'content': 'body'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        From                         To    Subject  \\\n",
      "0  (phillip.allen@enron.com)     (tim.belden@enron.com)              \n",
      "1  (phillip.allen@enron.com)  (john.lavorato@enron.com)        Re:   \n",
      "2  (phillip.allen@enron.com)   (leah.arsdall@enron.com)   Re: test   \n",
      "3  (phillip.allen@enron.com)    (randall.gay@enron.com)              \n",
      "4  (phillip.allen@enron.com)     (greg.piper@enron.com)  Re: Hello   \n",
      "\n",
      "                                                body  \n",
      "0                          Here is our forecast\\n\\n   \n",
      "1  Traveling to have a business meeting takes the...  \n",
      "2                     test successful.  way to go!!!  \n",
      "3  Randy,\\n\\n Can you send me a schedule of the s...  \n",
      "4                Let's shoot for Tuesday at 11:45.    \n"
     ]
    }
   ],
   "source": [
    "enron = enron[['From', 'To', 'Subject', 'body']]\n",
    "print(enron.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy # for lemmatization\n",
    "# for plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # the module 'gensim' has renamed to gensim_models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep NLTK stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# You have to install the word on your local machine , 3.84 GB!!\n",
    "# https://www.nltk.org/data.html\n",
    "stop_words = stopwords.words('english') # will cause error if not installed accordint to the instructions above\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'send', 'com']) # if replied the mail contains thes words -> remove them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\n",
      "\n",
      "As far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \n",
      "\n",
      "My suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enron.iloc[1]['body']) # displays info below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert email body to list\n",
    "data = enron.body.values.tolist()\n",
    "#data = enron.Subject.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize - break down each sentence into a list of words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['traveling', 'to', 'have', 'business', 'meeting', 'takes', 'the', 'fun', 'out', 'of', 'the', 'trip', 'especially', 'if', 'you', 'have', 'to', 'prepare', 'presentation', 'would', 'suggest', 'holding', 'the', 'business', 'plan', 'meetings', 'here', 'then', 'take', 'trip', 'without', 'any', 'formal', 'business', 'meetings', 'would', 'even', 'try', 'and', 'get', 'some', 'honest', 'opinions', 'on', 'whether', 'trip', 'is', 'even', 'desired', 'or', 'necessary', 'as', 'far', 'as', 'the', 'business', 'meetings', 'think', 'it', 'would', 'be', 'more', 'productive', 'to', 'try', 'and', 'stimulate', 'discussions', 'across', 'the', 'different', 'groups', 'about', 'what', 'is', 'working', 'and', 'what', 'is', 'not', 'too', 'often', 'the', 'presenter', 'speaks', 'and', 'the', 'others', 'are', 'quiet', 'just', 'waiting', 'for', 'their', 'turn', 'the', 'meetings', 'might', 'be', 'better', 'if', 'held', 'in', 'round', 'table', 'discussion', 'format', 'my', 'suggestion', 'for', 'where', 'to', 'go', 'is', 'austin', 'play', 'golf', 'and', 'rent', 'ski', 'boat', 'and', 'jet', 'ski', 'flying', 'somewhere', 'takes', 'too', 'much', 'time']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[data_words], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forwarded', 'by', 'phillip', 'allen', 'hou', 'ect', 'on', 'pm', 'from', 'julie_gomez', 'am', 'to', 'hunter_shively', 'hou', 'ect', 'ect', 'phillip', 'allen', 'hou', 'ect', 'ect', 'cc', 'subject', 'alliance', 'netback', 'worksheet', 'hello', 'men', 'have', 'attached', 'my', 'worksheet', 'in', 'case', 'you', 'want', 'to', 'review', 'the', 'data', 'while', 'am', 'on', 'holiday', 'thanks', 'julie']\n"
     ]
    }
   ],
   "source": [
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[200]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop_words, make bigrams and lemmatize\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words \n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner']) # this is how it used to be\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_lemmatized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary and corpus both are needed for (LDA) topic modeling\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = './mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "# corpus, dictionary and number of topics required for LDA\n",
    "# alpha and eta are hyperparameters that affect sparsity of the topics\n",
    "# chunksize is the number of documents to be used in each training chunk\n",
    "# update_every determines how often the model parameters should be updated\n",
    "# passes is the total number of training passes\n",
    "# Print the Keyword in the 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())# The weights reflect how important a keyword is to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model perplexity and topic coherence provide a convenient\n",
    "# measure to judge how good a given topic model is.\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook(sort=True)\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = pyLDAvis.gensim_models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
    "\n",
    "\n",
    "#ldamallet = pyLDAvis.gensim_models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a107e33ec9895a199db06dea1e1fb8d88e4830832973b5c2e63e39182c97baa2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
